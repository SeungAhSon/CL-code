{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vil/anaconda3/envs/llm2vec/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)\n",
    "\n",
    "MODEL_PATH = '../model/sheared_llama_1.3B/'\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(MODEL_PATH).to(DEVICE)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "# this function extracts the activation vectors for every layer during the forward pass of a prompt and saves them as .pkl files\n",
    "def process_dataset(dataset_name):\n",
    "    df = []\n",
    "    print(f\"Saving activations for {dataset_name} dataset!\")\n",
    "    if \"GoEmo\" in dataset_name:\n",
    "        df = pd.read_pickle('../datasets/pkl/go_emotions.pkl')\n",
    "    elif \"yelp\" in dataset_name:\n",
    "        df = pd.read_pickle('../datasets/pkl/yelp.pkl')\n",
    "        \n",
    "    actis, actis_train, actis_test = [],[],[]\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for index, row in tqdm(df.iterrows(), desc=f\"Iterating through all samples from the {DATASET} dataset and extracting the activations\", total=df.shape[0]):\n",
    "\n",
    "        # removing newlines from samples.\n",
    "        sentence = []\n",
    "        if \"GoEmo\" in dataset_name:\n",
    "            sentence = row['text'].replace('\\n', '')\n",
    "        else:\n",
    "            sentence = row['sample'].replace('\\n', '')\n",
    "        input_tokens = tokenizer(sentence, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "        # skip samples with more than 300 tokens, otherwise GPU runs out of memory\n",
    "        if len(input_tokens.input_ids) > 300: \n",
    "            continue\n",
    "        gen_text = model.forward(input_tokens.input_ids, output_hidden_states=True, return_dict=True)\n",
    "        hidden_states = []\n",
    "\n",
    "        #iterating over all layers and storing activations of the last token\n",
    "        for layer in gen_text['hidden_states']:\n",
    "            hidden_states.append(layer[0][-1].detach().cpu().numpy())\n",
    "\n",
    "        if DATASET == \"GoEmo\":\n",
    "            actis.append([index,row,hidden_states])\n",
    "        else:\n",
    "            # yelp store the labels in column 'sentiment', go emotion stores labels in 'labels' column.\n",
    "            actis.append([index, sentence, hidden_states, row['sentiment']])\n",
    "\n",
    "            i += 1\n",
    "            # save activations in batches\n",
    "            if i == 10000:\n",
    "                i = 0\n",
    "                with open(f'{PATH_TO_ACTIVATION_STORAGE}/{dataset_name}_activations_{j}.pkl', 'wb') as f:\n",
    "                    pickle.dump(actis, f)\n",
    "                del actis\n",
    "                del hidden_states\n",
    "                actis = []\n",
    "                j += 1\n",
    "    \n",
    "    if DATASET==\"GoEmo\":\n",
    "        actis_train = actis[0:4343] # training set\n",
    "        actis_test = actis[4343:4343+554] # test set\n",
    "        # we ignore the val set\n",
    "        with open(f'{PATH_TO_ACTIVATION_STORAGE}/{dataset_name}_activations_train.pkl', 'wb') as f:\n",
    "                pickle.dump(actis_train, f)\n",
    "        with open(f'{PATH_TO_ACTIVATION_STORAGE}/{dataset_name}_activations_test.pkl', 'wb') as f:\n",
    "                pickle.dump(actis_test, f)\n",
    "    else:    \n",
    "        # in case the number of samples is not dividable by 10000, we safe the rest\n",
    "        with open(f'{PATH_TO_ACTIVATION_STORAGE}/{dataset_name}_activations_{j}.pkl', 'wb') as f:\n",
    "                pickle.dump(actis, f)\n",
    "    \n",
    "    del actis\n",
    "    del hidden_states\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving activations for yelp dataset!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating through all samples from the yelp dataset and extracting the activations:   0%|          | 1343/542417 [00:31<3:31:52, 42.56it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DATASET\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myelp\u001b[39m\u001b[38;5;124m\"\u001b[39m:     PATH_TO_ACTIVATION_STORAGE \u00