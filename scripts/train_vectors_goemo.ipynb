{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import torch\n",
    "from transformers import set_seed\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import pickle\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from torch import cuda\n",
    "from pathlib import Path\n",
    "\n",
    "from utils import dataset_loader as dsl\n",
    "from utils.steering_layer import SteeringLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "cuda.empty_cache()\n",
    "DEVICE = torch.device('cuda:0')\n",
    "# print(device)\n",
    "\n",
    "SEED = 1337\n",
    "set_seed(SEED)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "INSERTION_LAYERS = [18, 19, 20] # best layers in our experiments\n",
    "MODEL_PATH = os.getenv(\"ALPACA_WEIGHTS_FOLDER\")\n",
    "TRAINED_STEERING_VECTOR_PATH = os.getenv(\"TRAINED_VECTORS_PATH_GoEmo\")\n",
    "Path(TRAINED_STEERING_VECTOR_PATH).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "MAX_NEW_TOKENS = 20\n",
    "TARGET_SENTENCE_COUNTER = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to save a trained steering vector\n",
    "def save_trained_steering_vector(steering_vector, target_sentence, final_loss, layer_of_interest, epoch_of_extraction, gen_text, label, TARGET_SENTENCE_COUNTER):\n",
    "    save_dict = {target_sentence: (steering_vector, layer_of_interest, final_loss, epoch_of_extraction, gen_text, label)}\n",
    "\n",
    "    with open(f\"{TRAINED_STEERING_VECTOR_PATH}/LLMB{str(INSERTION_LAYERS)}_{str(TARGET_SENTENCE_COUNTER)}.pkl\", 'wb') as fp:\n",
    "        pickle.dump(save_dict, fp)\n",
    "        print(f\"Steering vector for sentence \\\"{target_sentence}\\\" saved at {TRAINED_STEERING_VECTOR_PATH}/LLMB{str(INSERTION_LAYERS)}_{str(TARGET_SENTENCE_COUNTER)}.pkl\")\n",
    "\n",
    "df_goemo = dsl.load_goemo() # 54263 samples\n",
    "# filter out samples with a length over 50 due to time constraints.\n",
    "df = df_goemo[df_goemo['text'].str.len() < 50] # 19396 samples \n",
    "# print(\"GoEmo Dataset loaded\")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = LlamaForCausalLM.from_pretrained(MODEL_PATH, low_cpu_mem_usage=True)\n",
    "model.to(DEVICE)\n",
    "\n",
    "\n",
    "# Only compute gradients for steering vector\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "# Adding our custom steering layer to the model\n",
    "for insert_layer in INSERTION_LAYERS:\n",
    "    model.model.layers[insert_layer].mlp = SteeringLayer(model.model.layers[insert_layer].mlp)\n",
    "\n",
    "EPOCHS = 400\n",
    "learning_rate = 0.01\n",
    "decayRate = 0.96\n",
    "num_tokens_to_predict = 50\n",
    "current_lr = learning_rate\n",
    "for index, row in df.iterrows():\n",
    "    label = row[\"labels\"]\n",
    "    target = row[\"text\"]\n",
    "\n",
    "    # Get raw activations for target sentence\n",
    "    for insert_layer in INSERTION_LAYERS:\n",
    "        model.model.layers[insert_layer].mlp.add_steering = False\n",
    "    target_tokens = tokenizer(target, return_tensors=\"pt\").to(DEVICE)\n",
    "    model_output = model.forward(target_tokens.input_ids)\n",
    "    raw_activations = []\n",
    "    for insert_layer in INSERTION_LAYERS:\n",
    "        # model.model.layers[insert_layer].mlp.activations\n",
    "        model.model.layers[insert_layer].mlp.add_steering = True\n",
    "\n",
    "    # Init steering vector\n",
    "    for insert_layer in INSERTION_LAYERS:\n",
    "        model.model.layers[insert_layer].mlp.reset_steering_vector()\n",
    "        print(f\"Initial Steering Vector: {model.model.layers[insert_layer].mlp.steering_vector}\")\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "    custom_layers = []\n",
    "    for insert_layer in INSERTION_LAYERS:\n",
    "        custom_layers.append(model.model.layers[insert_layer].mlp.steering_vector)\n",
    "    adam_optim = optim.Adam(custom_layers, lr=learning_rate)\n",
    "    \n",
    "    \n",
    "    current_bleu = 0\n",
    "    epoch_of_extraction = 0\n",
    "    for current_epoch in range(0,EPOCHS):\n",
    "        if current_epoch >= 1: \n",
    "            current_bleu = BLEUscore\n",
    "        overall_loss = 0\n",
    "        target_tokens = tokenizer(target, return_tensors=\"pt\").to(DEVICE)\n",
    "        input_tokens = tokenizer(\"\", return_tensors=\"pt\").to(DEVICE)\n",
    "        gen_tokens = []\n",
    "\n",
    "        for j in range(len(target_tokens.input_ids[0])-1):\n",
    "            if j == 0:\n",
    "                model_output = model.forward(input_tokens.input_ids)\n",
    "                logits = model_output.logits\n",
    "                gen_tokens.append(np.argmax(logits.detach().cpu()))\n",
    "                past_key_vals = model_output.past_key_values\n",
    "                overall_loss += loss_fn(logits[0][0], target_tokens.input_ids[0][j+1])\n",
    "            else:\n",
    "                model_output = model.forward(torch.Tensor([[np.argmax(logits.detach().cpu())]]).type(torch.int64).to(DEVICE), past_key_values = past_key_vals)\n",
    "                logits = model_output.logits\n",
    "                gen_tokens.append(np.argmax(logits.detach().cpu()))\n",
    "                past_key_vals = model_output.past_key_values\n",
    "                overall_loss += loss_fn(logits[0][0], target_tokens.input_ids[0][j+1])\n",
    "\n",
    "\n",
    "        BLEUscore = nltk.translate.bleu_score.sentence_bleu([target.split()], \" \".join(tokenizer.batch_decode(gen_tokens)).split())\n",
    "\n",
    "        overall_loss.backward()\n",
    "        adam_optim.step()\n",
    "        \n",
    "        if overall_loss < 100:\n",
    "            for g in adam_optim.param_groups:\n",
    "                g[\"lr\"] = 0.01\n",
    "                current_lr = g[\"lr\"]\n",
    "                \n",
    "        epoch_of_extraction = current_epoch\n",
    "        final_loss = overall_loss\n",
    "        if gen_tokens == [b for b in target_tokens.input_ids[0][1:]]:\n",
    "            print(\"====================================================================================================\")\n",
    "            print(\"Matching steering vector found! Stopping training.\")\n",
    "            print(f\"Final Epoch {current_epoch}\\nTarget sentence: {target}\")\n",
    "            # print(f\"Final Steering Vector Gradient: {model.model.layers[INSERTION_LAYER].mlp.steering_vector.grad}\")\n",
    "            print(f\"Final generated text: {tokenizer.batch_decode(gen_tokens)}\")\n",
    "            print(f\"Final BLEU score: {BLEUscore}\")\n",
    "            print(f\"Loss: {overall_loss}\")\n",
    "            # print(f\"Final Steering Vector: {model.model.layers[INSERTION_LAYER].mlp.steering_vector}\")\n",
    "            # save_trained_steering_vector(model.model.layers[INSERTION_LAYER].mlp.steering_vector, target, label)\n",
    "            custom_layers = []\n",
    "            for insert_layer in INSERTION_LAYERS:\n",
    "                custom_layers.append(model.model.layers[insert_layer].mlp.steering_vector.data)\n",
    "            save_trained_steering_vector(custom_layers, \n",
    "                                target, final_loss, raw_activations, \n",
    "                                epoch_of_extraction, \" \".join(tokenizer.batch_decode(gen_tokens)), label, TARGET_SENTENCE_COUNTER)\n",
    "            TARGET_SENTENCE_COUNTER += 1\n",
    "            break\n",
    "        elif current_epoch == (EPOCHS-1):\n",
    "            print(\"====================================================================================================\")\n",
    "            print(f\"Epoch {current_epoch}\\nTarget sentence: {target}\")\n",
    "            # print(f\"Current Steering Vector Gradient: {model.model.layers[INSERTION_LAYER].mlp.steering_vector.grad}\")\n",
    "            print(f\"Current generated text: {tokenizer.batch_decode(gen_tokens)}\")\n",
    "            print(f\"Current BLEU score: {BLEUscore}\")\n",
    "            print(f\"Current learning rate: {current_lr}\")\n",
    "            print(f\"Loss: {overall_loss}\")\n",
    "            # print(f\"Updated Steering Vector: {model.model.layers[INSERTION_LAYER].mlp.steering_vector}\")\n",
    "            if BLEUscore > current_bleu:\n",
    "                custom_layers = []\n",
    "                for insert_layer in INSERTION_LAYERS:\n",
    "                    custom_layers.append(model.model.layers[insert_layer].mlp.steering_vector.data)\n",
    "                save_trained_steering_vector(custom_layers, \n",
    "                                    target, final_loss, raw_activations, \n",
    "                                    epoch_of_extraction, \" \".join(tokenizer.batch_decode(gen_tokens)), label, TARGET_SENTENCE_COUNTER)\n",
    "                TARGET_SENTENCE_COUNTER += 1\n",
    "        else:\n",
    "            print(\"====================================================================================================\")\n",
    "            print(f\"Epoch {current_epoch}\\nTarget sentence: {target}\")\n",
    "            # print(f\"Current Steering Vector Gradient: {model.model.layers[INSERTION_LAYER].mlp.steering_vector.grad}\")\n",
    "            print(f\"Current generated text: {tokenizer.batch_decode(gen_tokens)}\")\n",
    "            print(f\"Current BLEU score: {BLEUscore}\")\n",
    "            print(f\"Current learning rate: {current_lr}\")\n",
    "            print(f\"Loss: {overall_loss}\")\n",
    "            # print(f\"Updated Steering Vector: {model.model.layers[INSERTION_LAYER].mlp.steering_vector}\")\n",
    "            if BLEUscore > current_bleu:\n",
    "                custom_layers = []\n",
    "                for insert_layer in INSERTION_LAYERS:\n",
    "                    custom_layers.append(model.model.layers[insert_layer].mlp.steering_vector.data)\n",
    "                save_trained_steering_vector(custom_layers, \n",
    "                                    target, final_loss, raw_activations, \n",
    "                                    epoch_of_extraction, \" \".join(tokenizer.batch_decode(gen_tokens)), label, TARGET_SENTENCE_COUNTER)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
