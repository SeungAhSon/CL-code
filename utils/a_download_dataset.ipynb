{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Go_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_go_emotions(dataframe_path):\n",
    "  # 데이터셋 로드\n",
    "  dataset = load_dataset(\"google-research-datasets/go_emotions\")\n",
    "  dataset_train = dataset['train'].to_pandas()   \n",
    "  dataset_test = dataset['test'].to_pandas()\n",
    "  dataset_validation = dataset['validation'].to_pandas()\n",
    "  \n",
    "  dataset = pd.concat([dataset_train, dataset_test, dataset_validation], axis=0, ignore_index=True)\n",
    "  print(\"Orig Train #: \", len(dataset_train))\n",
    "  print(\"Orig Test  #:\", len(dataset_test))\n",
    "  print(\"Orig Valid #:\", len(dataset_validation))\n",
    "  print(\"Orig Total #:\", len(dataset))\n",
    "  #dataset.head()\n",
    "  \n",
    "  # 6가지 감정만 추출\n",
    "  only_ekman=True\n",
    "  if only_ekman :  \n",
    "    wanted_emotions = ['sadness', 'joy', 'fear', 'anger', 'surprise', 'disgust']\n",
    "    dataset_emotions = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion',\n",
    "                        'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment',\n",
    "                        'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism',\n",
    "                        'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
    "    base_emotion_indices = [dataset_emotions.index(emotion) for emotion in wanted_emotions]\n",
    "    print(\"base_emotion_indices\", base_emotion_indices)\n",
    "    \n",
    "    selected_samples = []\n",
    "    for i in range(len(dataset)):\n",
    "        df_tmp = dataset.iloc[i]\n",
    "        if len(df_tmp['labels']) == 1 and df_tmp['labels'][0] in base_emotion_indices:\n",
    "            selected_samples.append(df_tmp)\n",
    "\n",
    "    dataset = pd.DataFrame(selected_samples, columns=['text', 'labels', 'id']).reset_index(drop=True)\n",
    "    print(\"New Total #:\", len(dataset))\n",
    "    \n",
    "    dataset.to_pickle(dataframe_path)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig Train #:  43410\n",
      "Orig Test  #: 5427\n",
      "Orig Valid #: 5426\n",
      "Orig Total #: 54263\n",
      "base_emotion_indices [25, 17, 14, 2, 26, 11]\n",
      "New Total #: 5410\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>[2]</td>\n",
       "      <td>eezlygj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>[14]</td>\n",
       "      <td>ed7ypvh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...</td>\n",
       "      <td>[26]</td>\n",
       "      <td>edvnz26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fucking coward.</td>\n",
       "      <td>[2]</td>\n",
       "      <td>edk0z9k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stupidly stubborn / stubbornly stupid</td>\n",
       "      <td>[2]</td>\n",
       "      <td>edkh6qo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5405</th>\n",
       "      <td>I can't believe this team beat Winnipeg last w...</td>\n",
       "      <td>[26]</td>\n",
       "      <td>eeblypq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5406</th>\n",
       "      <td>I would be afraid he would come back and someh...</td>\n",
       "      <td>[14]</td>\n",
       "      <td>edew673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5407</th>\n",
       "      <td>It happens so much that it really scares me tbh.</td>\n",
       "      <td>[14]</td>\n",
       "      <td>ee2lna8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5408</th>\n",
       "      <td>Threats and promotion of violence</td>\n",
       "      <td>[14]</td>\n",
       "      <td>ef3gvik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5409</th>\n",
       "      <td>It's pretty dangerous when the state decides w...</td>\n",
       "      <td>[14]</td>\n",
       "      <td>edyrazk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5410 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text labels       id\n",
       "0                        WHY THE FUCK IS BAYLESS ISOING    [2]  eezlygj\n",
       "1                           To make her feel threatened   [14]  ed7ypvh\n",
       "2     OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...   [26]  edvnz26\n",
       "3                                       Fucking coward.    [2]  edk0z9k\n",
       "4                 Stupidly stubborn / stubbornly stupid    [2]  edkh6qo\n",
       "...                                                 ...    ...      ...\n",
       "5405  I can't believe this team beat Winnipeg last w...   [26]  eeblypq\n",
       "5406  I would be afraid he would come back and someh...   [14]  edew673\n",
       "5407   It happens so much that it really scares me tbh.   [14]  ee2lna8\n",
       "5408                  Threats and promotion of violence   [14]  ef3gvik\n",
       "5409  It's pretty dangerous when the state decides w...   [14]  edyrazk\n",
       "\n",
       "[5410 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_path = '../datasets/pkl/go_emotions.pkl'\n",
    "download_go_emotions(dataframe_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_yelp(dataframe_path):\n",
    "    #dataset already downloaded from https://github.com/shentianxiao/language-style-transfer/tree/master/data/yelp\n",
    "    \n",
    "    orig_path = \"../datasets/original/yelp/\"\n",
    "    yelp_dict = {\n",
    "        'sentiment.dev.0': [\"dev\", 0],\n",
    "        'sentiment.dev.1': [\"dev\", 1],\n",
    "        'sentiment.test.0': [\"test\", 0],\n",
    "        'sentiment.test.1': [\"test\", 1],\n",
    "        'sentiment.train.0': [\"train\", 0],\n",
    "        'sentiment.train.1': [\"train\", 1]\n",
    "    }\n",
    "    labels = []\n",
    "    dataset = []\n",
    "    sentences = []\n",
    "    files = [os.path.join(orig_path, f) for f in os.listdir(orig_path) if os.path.isfile(os.path.join(orig_path, f))]\n",
    "    \n",
    "    for elem in tqdm(files, desc=\"Preparing dataset files and saving as pkl\"): \n",
    "        file = os.path.basename(elem)\n",
    "        if not \"reference\" in file:\n",
    "            with open(elem, 'r') as f:\n",
    "                for l in f:\n",
    "                    l = l.strip()\n",
    "                    labels.append(yelp_dict[file][1])\n",
    "                    dataset.append(yelp_dict[file][0])\n",
    "                    sentences.append(l)\n",
    "\n",
    "    df = pd.DataFrame({\"dataset\": dataset, \"sentiment\": labels, \"sample\": sentences})\n",
    "    df = df.drop_duplicates(subset=[\"sample\"], keep=\"first\")  # sentences should be unique\n",
    "    df.to_pickle(dataframe_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../datasets/original/yelp/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dataframe_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../datasets/pkl/yelp.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdownload_yelp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m, in \u001b[0;36mdownload_yelp\u001b[0;34m(dataframe_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m dataset \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     15\u001b[0m sentences \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 16\u001b[0m files \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(orig_path, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(orig_path, f))]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m tqdm(files, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreparing dataset files and saving as pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m): \n\u001b[1;32m     19\u001b[0m     file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(elem)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../datasets/original/yelp/'"
     ]
    }
   ],
   "source": [
    "dataframe_path = '../datasets/pkl/yelp.pkl'\n",
    "download_yelp(dataframe_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
